{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6b8e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b71bcd121224f3db7736e94afe50cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TChu0702\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TChu0702\\.cache\\huggingface\\hub\\models--FacebookAI--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1045bb73444cb7896b1c3bc8b1f5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cf22e3d7ca40839c1ef3b9eb5fec64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab1c1b209d94cc68b314f413b6e6ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155ef6695b4a4102af62783e2f45f569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a1cc9ee2e341bab9435ff7adb323c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The package `optimum` is required to use Better Transformer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \n\u001b[0;32m     14\u001b[0m                                              torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bettertransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\lib\\site-packages\\transformers\\modeling_utils.py:4299\u001b[0m, in \u001b[0;36mPreTrainedModel.to_bettertransformer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4286\u001b[0m \u001b[38;5;124;03mConverts the model to use [PyTorch's native attention\u001b[39;00m\n\u001b[0;32m   4287\u001b[0m \u001b[38;5;124;03mimplementation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), integrated to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4296\u001b[0m \u001b[38;5;124;03m    [`PreTrainedModel`]: The model converted to BetterTransformer.\u001b[39;00m\n\u001b[0;32m   4297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_optimum_available():\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe package `optimum` is required to use Better Transformer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4301\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m optimum_version\n\u001b[0;32m   4303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(optimum_version) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.7.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mImportError\u001b[0m: The package `optimum` is required to use Better Transformer."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "#model_id = \"distilbert/distilgpt2\"\n",
    "\n",
    "model_id = \"FacebookAI/roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=torch.bfloat16)\n",
    "model.to_bettertransformer()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b8c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of a series of articles on the subject of the \"The Biggest Secret\" of the Internet is a new book about the Internet.\n",
      "\n",
      "\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret\n"
     ]
    }
   ],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer.encode(\n",
    "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
    "    response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
    "    return response\n",
    "\n",
    "\n",
    "question = \"What is a transformer?\"\n",
    "print(inference(question=question, context=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e175941a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma:2b',\n",
       " 'created_at': '2024-05-16T13:42:40.8569411Z',\n",
       " 'response': 'A transformer is an electrical device that transfers energy from one circuit to another through inductively coupled conductors. It is used to change the voltage or current level of a signal while maintaining the phase relationship between the two circuits. Transformers are commonly used in power distribution, communication, and industrial applications.',\n",
       " 'done': True,\n",
       " 'context': [106,\n",
       "  1645,\n",
       "  108,\n",
       "  1841,\n",
       "  603,\n",
       "  476,\n",
       "  46895,\n",
       "  235336,\n",
       "  107,\n",
       "  108,\n",
       "  106,\n",
       "  2516,\n",
       "  108,\n",
       "  235280,\n",
       "  46895,\n",
       "  603,\n",
       "  671,\n",
       "  16756,\n",
       "  6064,\n",
       "  674,\n",
       "  32363,\n",
       "  4134,\n",
       "  774,\n",
       "  974,\n",
       "  13341,\n",
       "  577,\n",
       "  2550,\n",
       "  1593,\n",
       "  11858,\n",
       "  28786,\n",
       "  34002,\n",
       "  93575,\n",
       "  235265,\n",
       "  1165,\n",
       "  603,\n",
       "  1671,\n",
       "  577,\n",
       "  2669,\n",
       "  573,\n",
       "  14623,\n",
       "  689,\n",
       "  2474,\n",
       "  2403,\n",
       "  576,\n",
       "  476,\n",
       "  9402,\n",
       "  2183,\n",
       "  22522,\n",
       "  573,\n",
       "  8342,\n",
       "  6843,\n",
       "  1865,\n",
       "  573,\n",
       "  1378,\n",
       "  37451,\n",
       "  235265,\n",
       "  128149,\n",
       "  708,\n",
       "  17203,\n",
       "  1671,\n",
       "  575,\n",
       "  2384,\n",
       "  7107,\n",
       "  235269,\n",
       "  9228,\n",
       "  235269,\n",
       "  578,\n",
       "  9365,\n",
       "  8557,\n",
       "  235265,\n",
       "  107,\n",
       "  108],\n",
       " 'total_duration': 26295940100,\n",
       " 'load_duration': 13799917400,\n",
       " 'prompt_eval_count': 14,\n",
       " 'prompt_eval_duration': 2145138000,\n",
       " 'eval_count': 58,\n",
       " 'eval_duration': 10335493000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.generate(model='gemma:2b', prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"LLMclinical.pdf\")\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = encoder.embed_query(\"RAG\")\n",
    "embeddings2 = encoder.embed_query(docs[0].page_content)\n",
    "print(np.dot(embeddings1, embeddings2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea19f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does COT prompting mean?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da022c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For this answer I used the following documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
