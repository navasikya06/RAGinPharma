{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eba15a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8200f66f9c74c16ae06fb487c6361d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6b8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "model_id = \"distilbert/distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "#model.to_bettertransformer()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b8c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of a series of articles on the subject of the \"The Biggest Secret\" of the Internet is a new book about the Internet.\n",
      "\n",
      "\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret of the Internet is a book about the Internet.\n",
      "The Biggest Secret\n"
     ]
    }
   ],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer.encode(\n",
    "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
    "    response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
    "    return response\n",
    "\n",
    "\n",
    "question = \"What is a transformer?\"\n",
    "print(inference(question=question, context=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ded826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"    \n",
    "    response = ollama.generate(model='gemma:2b', prompt=prompt)\n",
    "   \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e175941a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma:2b',\n",
       " 'created_at': '2024-05-22T02:41:02.0002534Z',\n",
       " 'response': 'A transformer is an electrical device that transfers energy from one circuit to another through induction. It is a passive device, meaning that it does not generate or consume electrical energy itself.\\n\\n**Key characteristics of a transformer:**\\n\\n* **Inductance:** The ability of a transformer to store and release energy in an alternating current circuit.\\n* **Voltage:** The amount of voltage supplied to the primary circuit.\\n* **Current:** The amount of current flowing through the primary and secondary circuits.\\n* **Power:** The rate at which energy is transferred from the primary to the secondary circuit.\\n* **Turns ratio:** The ratio of the number of turns in the primary to the number of turns in the secondary circuit.\\n\\n**How a transformer works:**\\n\\nA transformer works based on the principle of electromagnetic induction. When an alternating current is applied to the primary circuit, it creates a magnetic field. This magnetic field can induce an alternating current in the secondary circuit, even if there is no physical connection between the two circuits.\\n\\n**Applications of transformers:**\\n\\n* **Power distribution:** Transformers are used to distribute electricity from power plants to homes and businesses.\\n* **Communication:** Transformers are used in communication systems to increase or decrease the voltage of a signal.\\n* **Electronics:** Transformers are used in electronic devices, such as radios and computers.\\n* **Industrial applications:** Transformers are used in industrial machinery, such as motors and compressors.\\n\\n**Advantages of transformers:**\\n\\n* **High power ratings:** Transformers can handle large amounts of power.\\n* **High efficiency:** Transformers can transfer energy with high efficiency.\\n* **Isolation:** Transformers can isolate circuits from each other.',\n",
       " 'done': True,\n",
       " 'context': [106,\n",
       "  1645,\n",
       "  108,\n",
       "  1841,\n",
       "  603,\n",
       "  476,\n",
       "  46895,\n",
       "  235336,\n",
       "  107,\n",
       "  108,\n",
       "  106,\n",
       "  2516,\n",
       "  108,\n",
       "  235280,\n",
       "  46895,\n",
       "  603,\n",
       "  671,\n",
       "  16756,\n",
       "  6064,\n",
       "  674,\n",
       "  32363,\n",
       "  4134,\n",
       "  774,\n",
       "  974,\n",
       "  13341,\n",
       "  577,\n",
       "  2550,\n",
       "  1593,\n",
       "  33205,\n",
       "  235265,\n",
       "  1165,\n",
       "  603,\n",
       "  476,\n",
       "  30987,\n",
       "  6064,\n",
       "  235269,\n",
       "  6996,\n",
       "  674,\n",
       "  665,\n",
       "  1721,\n",
       "  780,\n",
       "  11941,\n",
       "  689,\n",
       "  35981,\n",
       "  16756,\n",
       "  4134,\n",
       "  5344,\n",
       "  235265,\n",
       "  109,\n",
       "  688,\n",
       "  2469,\n",
       "  10314,\n",
       "  576,\n",
       "  476,\n",
       "  46895,\n",
       "  66058,\n",
       "  109,\n",
       "  235287,\n",
       "  5231,\n",
       "  2230,\n",
       "  2421,\n",
       "  851,\n",
       "  66058,\n",
       "  714,\n",
       "  7374,\n",
       "  576,\n",
       "  476,\n",
       "  46895,\n",
       "  577,\n",
       "  4659,\n",
       "  578,\n",
       "  4236,\n",
       "  4134,\n",
       "  575,\n",
       "  671,\n",
       "  67420,\n",
       "  2474,\n",
       "  13341,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  67454,\n",
       "  66058,\n",
       "  714,\n",
       "  3619,\n",
       "  576,\n",
       "  14623,\n",
       "  18447,\n",
       "  577,\n",
       "  573,\n",
       "  7920,\n",
       "  13341,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  6846,\n",
       "  66058,\n",
       "  714,\n",
       "  3619,\n",
       "  576,\n",
       "  2474,\n",
       "  32866,\n",
       "  1593,\n",
       "  573,\n",
       "  7920,\n",
       "  578,\n",
       "  13752,\n",
       "  37451,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  10260,\n",
       "  66058,\n",
       "  714,\n",
       "  3974,\n",
       "  696,\n",
       "  948,\n",
       "  4134,\n",
       "  603,\n",
       "  21393,\n",
       "  774,\n",
       "  573,\n",
       "  7920,\n",
       "  577,\n",
       "  573,\n",
       "  13752,\n",
       "  13341,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  100396,\n",
       "  9537,\n",
       "  66058,\n",
       "  714,\n",
       "  9537,\n",
       "  576,\n",
       "  573,\n",
       "  1758,\n",
       "  576,\n",
       "  12722,\n",
       "  575,\n",
       "  573,\n",
       "  7920,\n",
       "  577,\n",
       "  573,\n",
       "  1758,\n",
       "  576,\n",
       "  12722,\n",
       "  575,\n",
       "  573,\n",
       "  13752,\n",
       "  13341,\n",
       "  235265,\n",
       "  109,\n",
       "  688,\n",
       "  2299,\n",
       "  476,\n",
       "  46895,\n",
       "  3598,\n",
       "  66058,\n",
       "  109,\n",
       "  235280,\n",
       "  46895,\n",
       "  3598,\n",
       "  3482,\n",
       "  611,\n",
       "  573,\n",
       "  12854,\n",
       "  576,\n",
       "  59714,\n",
       "  33205,\n",
       "  235265,\n",
       "  3194,\n",
       "  671,\n",
       "  67420,\n",
       "  2474,\n",
       "  603,\n",
       "  7936,\n",
       "  577,\n",
       "  573,\n",
       "  7920,\n",
       "  13341,\n",
       "  235269,\n",
       "  665,\n",
       "  18460,\n",
       "  476,\n",
       "  16472,\n",
       "  2725,\n",
       "  235265,\n",
       "  1417,\n",
       "  16472,\n",
       "  2725,\n",
       "  798,\n",
       "  37240,\n",
       "  671,\n",
       "  67420,\n",
       "  2474,\n",
       "  575,\n",
       "  573,\n",
       "  13752,\n",
       "  13341,\n",
       "  235269,\n",
       "  1693,\n",
       "  1013,\n",
       "  1104,\n",
       "  603,\n",
       "  793,\n",
       "  6915,\n",
       "  6653,\n",
       "  1865,\n",
       "  573,\n",
       "  1378,\n",
       "  37451,\n",
       "  235265,\n",
       "  109,\n",
       "  688,\n",
       "  35940,\n",
       "  576,\n",
       "  76581,\n",
       "  66058,\n",
       "  109,\n",
       "  235287,\n",
       "  5231,\n",
       "  10260,\n",
       "  7107,\n",
       "  66058,\n",
       "  128149,\n",
       "  708,\n",
       "  1671,\n",
       "  577,\n",
       "  37125,\n",
       "  19080,\n",
       "  774,\n",
       "  2384,\n",
       "  7652,\n",
       "  577,\n",
       "  11407,\n",
       "  578,\n",
       "  12065,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  46944,\n",
       "  66058,\n",
       "  128149,\n",
       "  708,\n",
       "  1671,\n",
       "  575,\n",
       "  9228,\n",
       "  5188,\n",
       "  577,\n",
       "  4740,\n",
       "  689,\n",
       "  16840,\n",
       "  573,\n",
       "  14623,\n",
       "  576,\n",
       "  476,\n",
       "  9402,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  122256,\n",
       "  66058,\n",
       "  128149,\n",
       "  708,\n",
       "  1671,\n",
       "  575,\n",
       "  16034,\n",
       "  9630,\n",
       "  235269,\n",
       "  1582,\n",
       "  685,\n",
       "  81110,\n",
       "  578,\n",
       "  25175,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  57000,\n",
       "  8557,\n",
       "  66058,\n",
       "  128149,\n",
       "  708,\n",
       "  1671,\n",
       "  575,\n",
       "  9365,\n",
       "  25491,\n",
       "  235269,\n",
       "  1582,\n",
       "  685,\n",
       "  35853,\n",
       "  578,\n",
       "  165320,\n",
       "  235265,\n",
       "  109,\n",
       "  688,\n",
       "  39857,\n",
       "  576,\n",
       "  76581,\n",
       "  66058,\n",
       "  109,\n",
       "  235287,\n",
       "  5231,\n",
       "  7978,\n",
       "  2384,\n",
       "  22658,\n",
       "  66058,\n",
       "  128149,\n",
       "  798,\n",
       "  6589,\n",
       "  2910,\n",
       "  15992,\n",
       "  576,\n",
       "  2384,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  7978,\n",
       "  12148,\n",
       "  66058,\n",
       "  128149,\n",
       "  798,\n",
       "  5853,\n",
       "  4134,\n",
       "  675,\n",
       "  1536,\n",
       "  12148,\n",
       "  235265,\n",
       "  108,\n",
       "  235287,\n",
       "  5231,\n",
       "  93539,\n",
       "  66058,\n",
       "  128149,\n",
       "  798,\n",
       "  53316,\n",
       "  37451,\n",
       "  774,\n",
       "  1853,\n",
       "  1156,\n",
       "  235265,\n",
       "  107,\n",
       "  108],\n",
       " 'total_duration': 57396397400,\n",
       " 'load_duration': 8093403300,\n",
       " 'prompt_eval_count': 14,\n",
       " 'prompt_eval_duration': 4087176000,\n",
       " 'eval_count': 338,\n",
       " 'eval_duration': 45195192000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.generate(model='gemma:2b', prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbe0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"files/LLMclinical.pdf\")\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d75aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7666c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elle Barral1, \n",
      "Christopher Semturs1, Alan Karthikesalingam1,5 ✉ & Vivek Natarajan1,5 ✉\n",
      "Large language models (LLMs) have demonstrated impressive capabilities, but the \n",
      "bar for clinical applications is high. Attempts to assess the clinical knowledge of \n",
      "models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six \n",
      "existing medical question answering datasets spanning professional medicine, \n",
      "research and consumer queries and a new dataset of medical questions\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594f26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c15cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02857219219547718\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = encoder.embed_query(\"RAG\")\n",
    "embeddings2 = encoder.embed_query(docs[0].page_content)\n",
    "print(np.dot(embeddings1, embeddings2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e568eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea19f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the abstract context within the prompt text.\n",
      "Chain-of-thought prompting. COT16 involves augmenting each \n",
      "few-shot example in the prompt with a step-by-step breakdown and \n",
      "a coherent set of intermediate reasoning steps towards the final  \n",
      "answer. The approach is designed to mimic the human thought process \n",
      "when solving problems that require multi-step computation and rea-\n",
      "soning. COT prompting can elicit reasoning abilities in sufficiently LLMs \n",
      "and dramatically improve performance on tasks such as mathe  matical \n",
      "Articleproblems16,62. Further, the appearance of such COT reasoning  \n",
      "appears to be an emergent ability28 of LLMs. COT prompting has been \n",
      "used to achieve breakthrough LLM performance on several STEM  \n",
      "benchmarks63.\n",
      "Many of the medical questions explored in this study involve complex \n",
      "multi-step reasoning, making them a good fit for COT prompting tech -\n",
      "niques. Together with clinicians, we crafted COT prompts to provide \n",
      "clear demonstrations on how to reason and answer the given medical \n",
      "questions. Examples of such prompts are detailed in Supplementary \n",
      "Nature | Vol 620 | 3 August 2023 | 175paths towards a particular answer, and sampling one path may not \n",
      "produce the most accurate result. This motivated the experiments \n",
      "with self-consistency, as discussed below. The COT prompts used are \n",
      "summarized in Supplementary Information, section 12. In addition, \n",
      "we also explored the use of non-medical COT prompts. The results \n",
      "presented in Supplementary Information, section 6 suggest that COT \n",
      "prompting is effective in priming the model to solve these types of \n",
      "problems rather than adding\n",
      " these results and the \n",
      "strong performance of the Flan-PaLM 540B model, we built on this \n",
      "model for downstream experiments and ablations. The scaling plots \n",
      "are provided in Supplementary Information, section 7.\n",
      "COT prompting\n",
      "Supplementary Table 2 summarizes the results from using COT prompt -\n",
      "ing and provides a comparison with the few-shot prompting strategy \n",
      "using the Flan-PaLM 540B model. We did not observe improvements \n",
      "using COT over the standard few-shot prompting strategy across the \n",
      "MedQA, MedMCQA and PubMedQA multiple-choice\n",
      "\n",
      "and craft few-shot prompts. Further research could expand the range \n",
      "of clinicians engaged in prompt construction and the selection of \n",
      "exemplar answers and thereby explore how variation in multiple axes of the types of clinician participating in this activity might affect LLM \n",
      "behaviour (such as clinician demographics, geography, specialism, \n",
      "lived experience and others).\n",
      "The pilot framework that we developed could be advanced using \n",
      "best practices for the design and validation of rating instruments from \n",
      "health, social and behavioural research32. This could entail finding \n",
      "additional rating items through participatory research and evalua\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What does COT prompting mean?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da022c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba18260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma:2b',\n",
       " 'created_at': '2024-05-17T07:08:51.8497022Z',\n",
       " 'response': 'The context does not provide a detailed definition of what COT prompting means, so I cannot answer this question from the provided context.',\n",
       " 'done': True,\n",
       " 'context': [106,\n",
       "  1645,\n",
       "  108,\n",
       "  15325,\n",
       "  573,\n",
       "  2113,\n",
       "  13614,\n",
       "  575,\n",
       "  573,\n",
       "  4807,\n",
       "  235269,\n",
       "  2734,\n",
       "  476,\n",
       "  11352,\n",
       "  3448,\n",
       "  577,\n",
       "  573,\n",
       "  2872,\n",
       "  235265,\n",
       "  108,\n",
       "  149,\n",
       "  2930,\n",
       "  235292,\n",
       "  139,\n",
       "  1175,\n",
       "  9949,\n",
       "  4807,\n",
       "  2819,\n",
       "  573,\n",
       "  18335,\n",
       "  2793,\n",
       "  235265,\n",
       "  108,\n",
       "  25482,\n",
       "  235290,\n",
       "  559,\n",
       "  235290,\n",
       "  47185,\n",
       "  103419,\n",
       "  235265,\n",
       "  91593,\n",
       "  235274,\n",
       "  235318,\n",
       "  18348,\n",
       "  35126,\n",
       "  574,\n",
       "  1853,\n",
       "  235248,\n",
       "  108,\n",
       "  53257,\n",
       "  235290,\n",
       "  23902,\n",
       "  3287,\n",
       "  575,\n",
       "  573,\n",
       "  18335,\n",
       "  675,\n",
       "  476,\n",
       "  4065,\n",
       "  235290,\n",
       "  1547,\n",
       "  235290,\n",
       "  8847,\n",
       "  25497,\n",
       "  578,\n",
       "  235248,\n",
       "  108,\n",
       "  235250,\n",
       "  63269,\n",
       "  1142,\n",
       "  576,\n",
       "  26840,\n",
       "  32346,\n",
       "  7161,\n",
       "  6643,\n",
       "  573,\n",
       "  2048,\n",
       "  139,\n",
       "  108,\n",
       "  13072,\n",
       "  235265,\n",
       "  714,\n",
       "  5688,\n",
       "  603,\n",
       "  6869,\n",
       "  577,\n",
       "  78763,\n",
       "  573,\n",
       "  3515,\n",
       "  3421,\n",
       "  2185,\n",
       "  235248,\n",
       "  108,\n",
       "  8999,\n",
       "  26149,\n",
       "  4552,\n",
       "  674,\n",
       "  2817,\n",
       "  3103,\n",
       "  235290,\n",
       "  8847,\n",
       "  37795,\n",
       "  578,\n",
       "  53245,\n",
       "  235290,\n",
       "  108,\n",
       "  1405,\n",
       "  574,\n",
       "  235265,\n",
       "  91593,\n",
       "  103419,\n",
       "  798,\n",
       "  121345,\n",
       "  32346,\n",
       "  24743,\n",
       "  575,\n",
       "  26055,\n",
       "  25599,\n",
       "  14816,\n",
       "  235248,\n",
       "  108,\n",
       "  639,\n",
       "  45532,\n",
       "  4771,\n",
       "  4665,\n",
       "  611,\n",
       "  13333,\n",
       "  1582,\n",
       "  685,\n",
       "  105858,\n",
       "  139,\n",
       "  60507,\n",
       "  235248,\n",
       "  108,\n",
       "  8320,\n",
       "  53102,\n",
       "  235274,\n",
       "  235318,\n",
       "  235269,\n",
       "  235318,\n",
       "  235284,\n",
       "  235265,\n",
       "  12214,\n",
       "  235269,\n",
       "  573,\n",
       "  10819,\n",
       "  576,\n",
       "  1582,\n",
       "  91593,\n",
       "  32346,\n",
       "  139,\n",
       "  108,\n",
       "  147501,\n",
       "  577,\n",
       "  614,\n",
       "  671,\n",
       "  137104,\n",
       "  7374,\n",
       "  235284,\n",
       "  235321,\n",
       "  576,\n",
       "  25599,\n",
       "  14816,\n",
       "  235265,\n",
       "  91593,\n",
       "  103419,\n",
       "  919,\n",
       "  1125,\n",
       "  235248,\n",
       "  108,\n",
       "  3909,\n",
       "  577,\n",
       "  7914,\n",
       "  58934,\n",
       "  629,\n",
       "  18622,\n",
       "  4665,\n",
       "  611,\n",
       "  3757,\n",
       "  64820,\n",
       "  139,\n",
       "  108,\n",
       "  31683,\n",
       "  7537,\n",
       "  235318,\n",
       "  235304,\n",
       "  235265,\n",
       "  108,\n",
       "  9691,\n",
       "  576,\n",
       "  573,\n",
       "  6910,\n",
       "  3920,\n",
       "  41316,\n",
       "  575,\n",
       "  736,\n",
       "  3320,\n",
       "  22395,\n",
       "  5766,\n",
       "  235248,\n",
       "  108,\n",
       "  6467,\n",
       "  235290,\n",
       "  8847,\n",
       "  32346,\n",
       "  235269,\n",
       "  3547,\n",
       "  1174,\n",
       "  476,\n",
       "  1426,\n",
       "  3806,\n",
       "  604,\n",
       "  91593,\n",
       "  103419,\n",
       "  2400,\n",
       "  728,\n",
       "  108,\n",
       "  29561,\n",
       "  235265,\n",
       "  32119,\n",
       "  675,\n",
       "  100716,\n",
       "  235269,\n",
       "  783,\n",
       "  57964,\n",
       "  91593,\n",
       "  73815,\n",
       "  577,\n",
       "  3658,\n",
       "  235248,\n",
       "  108,\n",
       "  5589,\n",
       "  60512,\n",
       "  611,\n",
       "  1368,\n",
       "  577,\n",
       "  3519,\n",
       "  578,\n",
       "  3448,\n",
       "  573,\n",
       "  2764,\n",
       "  6910,\n",
       "  235248,\n",
       "  108,\n",
       "  33460,\n",
       "  235265,\n",
       "  22510,\n",
       "  576,\n",
       "  1582,\n",
       "  73815,\n",
       "  708,\n",
       "  11352,\n",
       "  575,\n",
       "  75374,\n",
       "  235248,\n",
       "  108,\n",
       "  37044,\n",
       "  1420,\n",
       "  5195,\n",
       "  235248,\n",
       "  235318,\n",
       "  235284,\n",
       "  235276,\n",
       "  1420,\n",
       "  235248,\n",
       "  235304,\n",
       "  4826,\n",
       "  235248,\n",
       "  235284,\n",
       "  235276,\n",
       "  235284,\n",
       "  235304,\n",
       "  1420,\n",
       "  235248,\n",
       "  235274,\n",
       "  235324,\n",
       "  235308,\n",
       "  23720,\n",
       "  6643,\n",
       "  476,\n",
       "  3666,\n",
       "  3448,\n",
       "  235269,\n",
       "  578,\n",
       "  21100,\n",
       "  974,\n",
       "  3703,\n",
       "  1249,\n",
       "  780,\n",
       "  235248,\n",
       "  108,\n",
       "  78415,\n",
       "  573,\n",
       "  1546,\n",
       "  13650,\n",
       "  2196,\n",
       "  235265,\n",
       "  1417,\n",
       "  37762,\n",
       "  573,\n",
       "  13818,\n",
       "  235248,\n",
       "  108,\n",
       "  3041,\n",
       "  2011,\n",
       "  235290,\n",
       "  180702,\n",
       "  235269,\n",
       "  685,\n",
       "  11429,\n",
       "  3582,\n",
       "  235265,\n",
       "  714,\n",
       "  91593,\n",
       "  73815,\n",
       "  1671,\n",
       "  708,\n",
       "  235248,\n",
       "  108,\n",
       "  203819,\n",
       "  2012,\n",
       "  575,\n",
       "  75374,\n",
       "  5254,\n",
       "  235269,\n",
       "  4337,\n",
       "  236338,\n",
       "  235274,\n",
       "  235284,\n",
       "  235265,\n",
       "  878,\n",
       "  5081,\n",
       "  235269,\n",
       "  235248,\n",
       "  108,\n",
       "  966,\n",
       "  1170,\n",
       "  41316,\n",
       "  573,\n",
       "  1281,\n",
       "  576,\n",
       "  2173,\n",
       "  235290,\n",
       "  30227,\n",
       "  91593,\n",
       "  73815,\n",
       "  235265,\n",
       "  714,\n",
       "  3190,\n",
       "  235248,\n",
       "  108,\n",
       "  110460,\n",
       "  575,\n",
       "  75374,\n",
       "  5254,\n",
       "  235269,\n",
       "  4337,\n",
       "  236338,\n",
       "  235318,\n",
       "  9337,\n",
       "  674,\n",
       "  91593,\n",
       "  235248,\n",
       "  108,\n",
       "  39038,\n",
       "  574,\n",
       "  603,\n",
       "  7017,\n",
       "  575,\n",
       "  159891,\n",
       "  573,\n",
       "  2091,\n",
       "  577,\n",
       "  11560,\n",
       "  1450,\n",
       "  5088,\n",
       "  576,\n",
       "  235248,\n",
       "  108,\n",
       "  53102,\n",
       "  4644,\n",
       "  1178,\n",
       "  10480,\n",
       "  108,\n",
       "  1450,\n",
       "  3190,\n",
       "  578,\n",
       "  573,\n",
       "  235248,\n",
       "  108,\n",
       "  19265,\n",
       "  4665,\n",
       "  576,\n",
       "  573,\n",
       "  68321,\n",
       "  235290,\n",
       "  5064,\n",
       "  18622,\n",
       "  235248,\n",
       "  235308,\n",
       "  235310,\n",
       "  235276,\n",
       "  235305,\n",
       "  2091,\n",
       "  235269,\n",
       "  783,\n",
       "  6233,\n",
       "  611,\n",
       "  736,\n",
       "  235248,\n",
       "  108,\n",
       "  2516,\n",
       "  604,\n",
       "  47306,\n",
       "  13818,\n",
       "  578,\n",
       "  841,\n",
       "  49995,\n",
       "  235265,\n",
       "  714,\n",
       "  38591,\n",
       "  28921,\n",
       "  235248,\n",
       "  108,\n",
       "  895,\n",
       "  4646,\n",
       "  575,\n",
       "  75374,\n",
       "  5254,\n",
       "  235269,\n",
       "  4337,\n",
       "  236338,\n",
       "  235324,\n",
       "  235265,\n",
       "  108,\n",
       "  141619,\n",
       "  103419,\n",
       "  108,\n",
       "  55749,\n",
       "  5707,\n",
       "  236338,\n",
       "  235284,\n",
       "  86377,\n",
       "  573,\n",
       "  3190,\n",
       "  774,\n",
       "  2177,\n",
       "  91593,\n",
       "  18335,\n",
       "  728,\n",
       "  108,\n",
       "  574,\n",
       "  578,\n",
       "  6572,\n",
       "  476,\n",
       "  11482,\n",
       "  675,\n",
       "  573,\n",
       "  2619,\n",
       "  235290,\n",
       "  23902,\n",
       "  103419,\n",
       "  9803,\n",
       "  235248,\n",
       "  108,\n",
       "  1442,\n",
       "  573,\n",
       "  68321,\n",
       "  235290,\n",
       "  5064,\n",
       "  18622,\n",
       "  235248,\n",
       "  235308,\n",
       "  235310,\n",
       "  235276,\n",
       "  235305,\n",
       "  2091,\n",
       "  235265,\n",
       "  1448,\n",
       "  1498,\n",
       "  780,\n",
       "  20483,\n",
       "  18806,\n",
       "  235248,\n",
       "  108,\n",
       "  1442,\n",
       "  91593,\n",
       "  1163,\n",
       "  573,\n",
       "  5039,\n",
       "  2619,\n",
       "  235290,\n",
       "  23902,\n",
       "  103419,\n",
       "  9803,\n",
       "  4492,\n",
       "  573,\n",
       "  235248,\n",
       "  108,\n",
       "  4567,\n",
       "  39213,\n",
       "  235269,\n",
       "  2934,\n",
       "  9186,\n",
       "  39213,\n",
       "  578,\n",
       "  74552,\n",
       "  39213,\n",
       "  6733,\n",
       "  235290,\n",
       "  21062,\n",
       "  109,\n",
       "  639,\n",
       "  18038,\n",
       "  2619,\n",
       "  235290,\n",
       "  23902,\n",
       "  73815,\n",
       "  235265,\n",
       "  12214,\n",
       "  3679,\n",
       "  1538,\n",
       "  6725,\n",
       "  573,\n",
       "  3001,\n",
       "  235248,\n",
       "  108,\n",
       "  559,\n",
       "  100716,\n",
       "  15504,\n",
       "  575,\n",
       "  18335,\n",
       "  6584,\n",
       "  578,\n",
       "  573,\n",
       "  8492,\n",
       "  576,\n",
       "  235248,\n",
       "  108,\n",
       "  108569,\n",
       "  10523,\n",
       "  578,\n",
       "  22513,\n",
       "  15370,\n",
       "  1368,\n",
       "  16679,\n",
       "  575,\n",
       "  6733,\n",
       "  37261,\n",
       "  576,\n",
       "  573,\n",
       "  5088,\n",
       "  576,\n",
       "  144880,\n",
       "  28049,\n",
       "  575,\n",
       "  736,\n",
       "  5640,\n",
       "  2613,\n",
       "  8964,\n",
       "  629,\n",
       "  18622,\n",
       "  235248,\n",
       "  108,\n",
       "  133425,\n",
       "  591,\n",
       "  18505,\n",
       "  685,\n",
       "  144880,\n",
       "  93361,\n",
       "  235269,\n",
       "  53206,\n",
       "  235269,\n",
       "  3186,\n",
       "  2285,\n",
       "  235269,\n",
       "  235248,\n",
       "  108,\n",
       "  60394,\n",
       "  3281,\n",
       "  578,\n",
       "  3588,\n",
       "  846,\n",
       "  108,\n",
       "  651,\n",
       "  18576,\n",
       "  15087,\n",
       "  674,\n",
       "  783,\n",
       "  6990,\n",
       "  1538,\n",
       "  614,\n",
       "  11303,\n",
       "  2177,\n",
       "  235248,\n",
       "  108,\n",
       "  8351,\n",
       "  12317,\n",
       "  604,\n",
       "  573,\n",
       "  2480,\n",
       "  578,\n",
       "  20624,\n",
       "  576,\n",
       "  12613,\n",
       "  18159,\n",
       "  774,\n",
       "  235248,\n",
       "  108,\n",
       "  13859,\n",
       "  235269,\n",
       "  3127,\n",
       "  578,\n",
       "  109155,\n",
       "  3679,\n",
       "  235304,\n",
       "  235284,\n",
       "  235265,\n",
       "  1417,\n",
       "  1538,\n",
       "  94658,\n",
       "  10488,\n",
       "  235248,\n",
       "  108,\n",
       "  45547,\n",
       "  12613,\n",
       "  5100,\n",
       "  1593,\n",
       "  142923,\n",
       "  3679,\n",
       "  578,\n",
       "  41536,\n",
       "  2826,\n",
       "  108,\n",
       "  235265,\n",
       "  108,\n",
       "  149,\n",
       "  9413,\n",
       "  235292,\n",
       "  2439,\n",
       "  1721,\n",
       "  91593,\n",
       "  103419,\n",
       "  2714,\n",
       "  235336,\n",
       "  107,\n",
       "  108,\n",
       "  106,\n",
       "  2516,\n",
       "  108,\n",
       "  651,\n",
       "  4807,\n",
       "  1721,\n",
       "  780,\n",
       "  3658,\n",
       "  476,\n",
       "  11352,\n",
       "  10165,\n",
       "  576,\n",
       "  1212,\n",
       "  91593,\n",
       "  103419,\n",
       "  3454,\n",
       "  235269,\n",
       "  712,\n",
       "  590,\n",
       "  2952,\n",
       "  3448,\n",
       "  736,\n",
       "  2872,\n",
       "  774,\n",
       "  573,\n",
       "  4646,\n",
       "  4807,\n",
       "  235265,\n",
       "  107,\n",
       "  108],\n",
       " 'total_duration': 109482978100,\n",
       " 'load_duration': 11498373900,\n",
       " 'prompt_eval_count': 651,\n",
       " 'prompt_eval_duration': 93155949000,\n",
       " 'eval_count': 26,\n",
       " 'eval_duration': 4809612000}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For this answer I used the following documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
